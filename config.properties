# Corresponding properties in the config.properties file:
# Default settings
[DEFAULT]=
#limit trainig data to 10 and 2 epoch, batch size = 8
development_mode=False
retrain_model=True
profiling_enabled=True
#False will also will remove the cash
use_dataset_cache=False
#mixe sample training data
#sampler=RollingSubsetSampler
subset_fraction=0.2
#sampler=CurriculumSampler
curriculum_phase_count=5
curriculum_unlock_every=1
#This will set shuffle to True
sampler=None
#Options: %of data reshuffle. also will invalidate the cache. 0 to disable. Default 10%
solver=OptimizedBacktrackingMazeSolver
#solver = PledgeMazeSolver
#remote_execution = False
#Oprions: models = GRU, LSTM, RNN
#avoid wall collisions
wall_penalty=0.0
use_attention=True
models=LSTM
# Standard architecture:  4:local context, 2: relative coordinates, 1: steps in the solution
input_size=7
# 5 directions plus a signal for reaching the maze exit. Only use din training.
output_size=3
##Traning performance
training_samples=500000
batch_size=32
#Options: Windows 5-6 Linux:12-16
max_num_workers=8
#Automatic, if not defined
dataloader_workers=4
optimizer_type=Adam
scheduler_type=plateau
##Netowworm meta parameters:
hidden_size=128
num_layers=2
# Increase to 20 for final results.!!!!
num_epochs=40
patience=5
#Options 0.0001, 0.0005
learning_rate=0.0001
weight_decay=0.001
# Options: 0.5, 0.7, 0.9
lr_factor=0.5
# Options: 0.02, 0,01, 0.05
improvement_threshold=0.002
#Solution depth
max_steps=100
[LLM]=
#Options: Ollama, ChsatGPT,DeepSeek
provider=ChatGPT
algorithm=Backtracking
#Options: gpt-4o, gpt-4o-mini, ,gpt-4-turbo,gpt-4.1 / deepseek-chat
model_name=gpt-4.1
temperature=0.5
# RNN Model settings
[RNN]=
# GRU Model settings
[GRU]=
# LSTM Model settings
[LSTM]=
# Maze sSettings. Only even numbers
[MAZE]=
min_size=5
max_size=18
loop_probability=0.02
num_mazes=500000
[GENETIC]=
#options: 600-5000
max_population_size=2000
#options: 0.8 (0.7-0.9)
crossover_rate=0.8
#options: 0.2 - 0.4. max mutaiton rate for the gradiate approcah.
mutation_rate=0.2
generations=150
# tournament among top x only:
tournament_count=50
#options: 1-2. 0 -> disable
elitism_count=2
#options: 0 -> disable
diversity_infusion=0.1
## Fitness Configuration Parameters
#diverese genes
path_diversity_bonus=5
# Diversity penalty controls discouraging identical chromosomes:
# effectiveness = max(0, L*diversity_penalty_threshold - mean_hamming_distance_to_others)
diversity_penalty_weight=1
diversity_penalty_threshold=0
# Weight applied as a penalty for backtracking directly to the previous position.
# Penalize stepping back to the immediate previous cell
backtrack_penalty_weight=0.5
# Weight for penalizing the distance from the maze exit at the endpoint.
# Penalize being far from exit (Manhattan distance * this weight)
distance_penalty_weight=0.5
# Bonus weight for recovery from dead-ends (progress after hitting a penalty).
# Bonus if a chromosome recovers after making invalid moves (first valid move after invalids)
recover_bonus_weight=5
# Weight for rewarding the number of unique tiles visited.
# How much we reward visiting distinct cells
exploration_weight=2
# Penalty weight for invalid moves (e.g., hitting a wall).
# Discourages paths that attempt to move into unreachable areas of the maze.
invalid_move_penalty=2
# Penalize revisiting previously visited cells (non-adjacent return / loops)
loop_penalty_weight=10
# Small reward for ending closer to exit via BFS distance map; increase to make ?getting closer? matter.
bfs_distance_reward_weight=5
# Bonus for reaching the exit; larger => strong pressure for exact solutions.
# The formula uses (max_steps - steps) * exit_weight when exit reached.
# ===========================
# MUTATION SCHEDULING
# ===========================
# Gene-position dependent scaling of mutation probability (start of chromosome -> end)
mutation_start_multiplier=0.5
mutation_stop_multiplier=1.5
mutation_rate_floor=0.02
mutation_rate_ceiling=0.25
exit_weight=10
species_distance_threshold=0.25
#multithread. option: 0 for single thread
max_workers=0
random_seed=42
# how long to wait before abandoning training?
patience=30
early_stopping_threshold=0.02
evolution_chromosomes=10
[FILES]=
APP_PATH=.
OUTPUT=output/
INPUT=input/
GRU_MODEL=gru_model.pth
LSTM_MODEL=lstm_model.pth
RNN_MODEL=rnn_model.pth
TRAINING_MAZES=training_mazes.h5
VALIDATION_MAZES=validation_mazes.h5
MAZES=mazes.h5
LOSS_DATA=loss_data.csv
[MONITORING]=
visualization_mode=video
save_solution_movie=True
save_evolution_movie=True
wandb=False
dashboard=True
tensorboard=True
save_neural_network_diagram=False
save_last_loss_chart=True
generate_weights=True
save_mazes_as_pdf=True
generate_activations=True
print_mazes=True
sample_step=10
#options video, gif, none