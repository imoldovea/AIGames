# Corresponding properties in the config.properties file:
# Default settings
[DEFAULT]=
#limit trainig data to 10 and 2 epoch, batch size = 8
development_mode=False
retrain_model=True
profiling_enabled=True
#False will also will remove the cash
use_dataset_cache=False
#mixe sample training data
#sampler=RollingSubsetSampler
subset_fraction=0.2
#sampler=CurriculumSampler
curriculum_phase_count=5
curriculum_unlock_every=1
#This will set shuffle to True
sampler=None
#Options: %of data reshuffle. also will invalidate the cache. 0 to disable. Default 10%
solver=OptimizedBacktrackingMazeSolver
#solver = PledgeMazeSolver
#remote_execution = False
#Oprions: models = GRU, LSTM, RNN
#avoid wall collisions
wall_penalty=0.0
use_attention=True
models=LSTM
# Standard architecture:  4:local context, 2: relative coordinates, 1: steps in the solution
input_size=7
# 5 directions plus a signal for reaching the maze exit. Only use din training.
output_size=3
##Traning performance
training_samples=500000
batch_size=32
#Options: Windows 5-6 Linux:12-16
max_num_workers=8
#Automatic, if not defined
dataloader_workers=4
optimizer_type=Adam
scheduler_type=plateau
##Netowworm meta parameters:
hidden_size=128
num_layers=2
#Options: Balanced exploration: 2.0 / 5.0, Emphasize exit sharply: 5.0 / 10.0, Treat exit as primary goal: 10.0 / 20.0
exit_weight=0.1
# Increase to 20 for final results.!!!!
num_epochs=40
patience=5
#Options 0.0001, 0.0005
learning_rate=0.0001
weight_decay=0.001
# Options: 0.5, 0.7, 0.9
lr_factor=0.5
# Options: 0.02, 0,01, 0.05
improvement_threshold=0.002
#Solution depth
max_steps=80
[LLM]=
#Options: Ollama, ChsatGPT,DeepSeek
provider=ChatGPT
algorithm=Backtracking
#Options: gpt-4o, gpt-4o-mini / deepseek-chat
model_name=gpt-4o-mini
temperature=0
max_tokens=30
# RNN Model settings
[RNN]=
# GRU Model settings
[GRU]=
# LSTM Model settings
[LSTM]=
# Maze sSettings. Only even numbers
[MAZE]=
min_size=5
max_size=18
loop_probability=0.02
num_mazes=500000
[GENETIC]=
#options: 100-300
max_population_size=500
#options: 0.8 (0.7-0.9)
crossover_rate=0.9
#options: 0.2 - 0.4
mutation_rate=0.2
generations=2000
#options: 1-2. 0 -> disable
elitism_count=2
#options: 0 -> disable
diversity_infusion=0.03
#option: 0.1-1.0. 0 -> disable
diversity_penalty_weight=0.2
#option 0.3, 0.5 strict, 0-> disable
diversity_penalty_threshold=0.1
#multithread. option: 0 for single thread
max_workers=1
[FILES]=
APP_PATH=.
OUTPUT=output/
INPUT=input/
GRU_MODEL=gru_model.pth
LSTM_MODEL=lstm_model.pth
RNN_MODEL=rnn_model.pth
TRAINING_MAZES=training_mazes.h5
VALIDATION_MAZES=validation_mazes.h5
MAZES=mazes.h5
LOSS_DATA=loss_data.csv
[MONITORING]=
wandb=False
dashboard=True
tensorboard=True
save_neural_network_diagram=False
save_last_loss_chart=True
generate_weights=True
save_mazes_as_pdf=True
generate_activations=True
print_mazes=True
save_solution_movie=True